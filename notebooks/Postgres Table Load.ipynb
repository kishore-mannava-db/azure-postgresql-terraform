{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7177cd3c-de20-4d5c-b006-05cd118f7419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef6b9707-f45b-4bee-8c7c-53d3133f2e6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90ae1260-7784-4307-a18b-6941e7cf9eb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "jdbc_url = \"jdbc:postgresql://kishoremannava-pgserver.postgres.database.azure.com:5432/postgres\"\n",
    "username = \"mypgadmin\"\n",
    "password = \"YourSecurePassword123!\"\n",
    "\n",
    "def get_connection():\n",
    "    connection = psycopg2.connect(\n",
    "        dbname=\"postgres\",\n",
    "        user=username,\n",
    "        password=password,\n",
    "        host=\"kishoremannava-pgserver.postgres.database.azure.com\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "    connection.set_session(readonly=False, autocommit=True)\n",
    "    return connection\n",
    "\n",
    "def drop_tables(cursor, table_names, suffix):\n",
    "    for table in table_names:\n",
    "        print(f\"Dropping iteration {table}\")\n",
    "        drop_table_sql = f\"DROP TABLE IF EXISTS {table}_{suffix} CASCADE\"\n",
    "        print(drop_table_sql)\n",
    "        cursor.execute(drop_table_sql)\n",
    "\n",
    "def copy_table_structure(table_names, source_schema, suffix):\n",
    "    for table in table_names:\n",
    "        #print(\"readonly\", spark.conf.get(\"spark.sql.sources.readOnly\"))\n",
    "        df = spark.sql(f\"SELECT * FROM {source_schema}.{table} WHERE 1=0\")\n",
    "        df.write.format(\"jdbc\") \\\n",
    "            .option(\"url\", jdbc_url) \\\n",
    "            .option(\"user\", username) \\\n",
    "            .option(\"password\", password) \\\n",
    "            .option(\"dbtable\", f\"{table}_{suffix}\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save()\n",
    "\n",
    "def set_tables_unlogged(cursor, table_names, suffix):\n",
    "    for table in table_names:\n",
    "        alter_table_sql = f\"ALTER TABLE {table}_{suffix} SET UNLOGGED\"\n",
    "        print(alter_table_sql)\n",
    "        cursor.execute(alter_table_sql)\n",
    "\n",
    "def copy_csv_to_postgres_table(suffix, file_path, table_name):\n",
    "    connection = get_connection()\n",
    "    try:\n",
    "        new_table_name = f\"{table_name}_{suffix}\"\n",
    "        with open(file_path, 'r') as f:\n",
    "            cursor = connection.cursor()\n",
    "            copy_sql = f\"COPY {new_table_name} FROM STDIN WITH CSV HEADER\"\n",
    "            print(copy_sql)\n",
    "            cursor.copy_expert(sql=copy_sql, file=f)\n",
    "    finally:\n",
    "        connection.close()\n",
    "\n",
    "def optimize_tables(cursor, suffix):\n",
    "    tables_columns = {\n",
    "        \"call_center\": \"cc_call_center_sk\",\n",
    "        \"catalog_page\": \"cp_catalog_page_sk\",\n",
    "        \"catalog_returns\": \"cr_item_sk\",\n",
    "        \"customer\": \"c_customer_sk\",\n",
    "        \"customer_address\": \"ca_address_sk\",\n",
    "        \"customer_demographics\": \"cd_demo_sk\",\n",
    "        \"household_demographics\": \"hd_demo_sk\",\n",
    "        \"income_band\": \"ib_income_band_sk\",\n",
    "        \"inventory\": \"inv_item_sk\",\n",
    "        \"item\": \"i_item_sk\",\n",
    "        \"promotion\": \"p_promo_sk\",\n",
    "        \"reason\": \"r_reason_sk\",\n",
    "        \"store\": \"s_store_sk\",\n",
    "        \"store_returns\": \"sr_item_sk\",\n",
    "        \"store_sales\": \"ss_item_sk\",\n",
    "        \"time_dim\": \"t_time_sk\",\n",
    "        \"warehouse\": \"w_warehouse_sk\",\n",
    "        \"web_page\": \"wp_web_page_sk\",\n",
    "        \"web_returns\": \"wr_item_sk\",\n",
    "        \"web_sales\": \"ws_item_sk\",\n",
    "        \"catalog_sales\": \"cs_item_sk\",\n",
    "        \"web_site\": \"web_site_sk\"\n",
    "    }\n",
    "    for table, column in tables_columns.items():\n",
    "        cursor.execute(f\"\"\"\n",
    "            DO $$ BEGIN\n",
    "            IF NOT EXISTS (\n",
    "                SELECT 1 FROM pg_indexes\n",
    "                WHERE tablename = '{table}_{suffix}' AND indexname = 'idx_{column}_{suffix}'\n",
    "            ) THEN\n",
    "                EXECUTE 'CREATE INDEX idx_{column}_{suffix} ON {table}_{suffix} ({column})';\n",
    "            END IF;\n",
    "            END $$;\n",
    "        \"\"\")\n",
    "        cursor.execute(f\"ANALYZE {table}_{suffix} ({column})\")\n",
    "\n",
    "def run_iterations(iterations, table_names, source_schema):\n",
    "    for suffix in range(1, iterations + 1):\n",
    "        connection = get_connection()\n",
    "        cursor = connection.cursor()\n",
    "        try:\n",
    "            print(f\"Running iteration {suffix}\")\n",
    "            copy_table_structure(table_names, source_schema, suffix)\n",
    "            set_tables_unlogged(cursor, table_names, suffix)\n",
    "\n",
    "            table_paths = [\n",
    "                (table_name, f\"/Volumes/kishoremannava/default/tpcds/tpcds_sf100_csv/{table_name}\")\n",
    "                for table_name in table_names if table_name != \"_sqldf\"\n",
    "            ]\n",
    "\n",
    "            for table_name, table_path in table_paths:\n",
    "                print(\"tempFilePath:\", table_path)\n",
    "                csv_files = [os.path.join(table_path, f) for f in os.listdir(table_path) if f.endswith(\".csv\")]\n",
    "                from pyspark.sql.functions import lit\n",
    "\n",
    "                tasks = [(suffix, file_path, table_name) for file_path in csv_files]\n",
    "                tasks_df = spark.createDataFrame(tasks, [\"suffix\", \"file_path\", \"table_name\"])\n",
    "\n",
    "                def copy_task(row):\n",
    "                    copy_csv_to_postgres_table(row['suffix'], row['file_path'], row['table_name'])\n",
    "\n",
    "                tasks_df.foreach(copy_task)\n",
    "        finally:\n",
    "            cursor.close()\n",
    "            connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63b45b7f-c892-496c-9a77-57f5fa573e79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running iteration 1\nALTER TABLE call_center_1 SET UNLOGGED\nALTER TABLE catalog_page_1 SET UNLOGGED\nALTER TABLE catalog_returns_1 SET UNLOGGED\nALTER TABLE catalog_sales_1 SET UNLOGGED\nALTER TABLE customer_1 SET UNLOGGED\nALTER TABLE customer_address_1 SET UNLOGGED\nALTER TABLE customer_demographics_1 SET UNLOGGED\nALTER TABLE date_dim_1 SET UNLOGGED\nALTER TABLE household_demographics_1 SET UNLOGGED\nALTER TABLE income_band_1 SET UNLOGGED\nALTER TABLE inventory_1 SET UNLOGGED\nALTER TABLE item_1 SET UNLOGGED\nALTER TABLE promotion_1 SET UNLOGGED\nALTER TABLE reason_1 SET UNLOGGED\nALTER TABLE ship_mode_1 SET UNLOGGED\nALTER TABLE store_1 SET UNLOGGED\nALTER TABLE store_returns_1 SET UNLOGGED\nALTER TABLE store_sales_1 SET UNLOGGED\nALTER TABLE time_dim_1 SET UNLOGGED\nALTER TABLE warehouse_1 SET UNLOGGED\nALTER TABLE web_page_1 SET UNLOGGED\nALTER TABLE web_returns_1 SET UNLOGGED\nALTER TABLE web_sales_1 SET UNLOGGED\nALTER TABLE web_site_1 SET UNLOGGED\ntempFilePath: /Volumes/kishoremannava/default/tpcds/tpcds_sf100_csv/call_center\ntempFilePath: /Volumes/kishoremannava/default/tpcds/tpcds_sf100_csv/catalog_page\ntempFilePath: /Volumes/kishoremannava/default/tpcds/tpcds_sf100_csv/catalog_returns\ntempFilePath: /Volumes/kishoremannava/default/tpcds/tpcds_sf100_csv/catalog_sales\ntempFilePath: /Volumes/kishoremannava/default/tpcds/tpcds_sf100_csv/customer\ntempFilePath: /Volumes/kishoremannava/default/tpcds/tpcds_sf100_csv/customer_address\ntempFilePath: /Volumes/kishoremannava/default/tpcds/tpcds_sf100_csv/customer_demographics\ntempFilePath: /Volumes/kishoremannava/default/tpcds/tpcds_sf100_csv/date_dim\ntempFilePath: /Volumes/kishoremannava/default/tpcds/tpcds_sf100_csv/household_demographics\ntempFilePath: /Volumes/kishoremannava/default/tpcds/tpcds_sf100_csv/income_band\ntempFilePath: /Volumes/kishoremannava/default/tpcds/tpcds_sf100_csv/inventory\ntempFilePath: /Volumes/kishoremannava/default/tpcds/tpcds_sf100_csv/item\ntempFilePath: /Volumes/kishoremannava/default/tpcds/tpcds_sf100_csv/promotion\ntempFilePath: /Volumes/kishoremannava/default/tpcds/tpcds_sf100_csv/reason\ntempFilePath: /Volumes/kishoremannava/default/tpcds/tpcds_sf100_csv/ship_mode\ntempFilePath: /Volumes/kishoremannava/default/tpcds/tpcds_sf100_csv/store\ntempFilePath: /Volumes/kishoremannava/default/tpcds/tpcds_sf100_csv/store_returns\ntempFilePath: /Volumes/kishoremannava/default/tpcds/tpcds_sf100_csv/store_sales\n"
     ]
    }
   ],
   "source": [
    "catalog = \"kishoremannava\"\n",
    "schema = \"tpcds_sf100_delta\"\n",
    "unity_catalog_tables = spark.sql(f\"SHOW TABLES IN {catalog}.{schema}\")\n",
    "table_names = [row.tableName for row in unity_catalog_tables.select(\"tableName\").collect()]\n",
    "source_schema = f\"{catalog}.{schema}\"\n",
    "# copy_table_structure(table_names, source_schema, 1)\n",
    "run_iterations(5, table_names, source_schema)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "- Postgres Table Operations Script",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}